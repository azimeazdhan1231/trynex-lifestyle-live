# AWS Database Migration Guide for Trynex Lifestyle

## Current Setup
- **Database**: Supabase PostgreSQL (AWS Singapore pooler)
- **Performance**: 2000ms+ queries
- **Status**: Optimizing with connection pooling + caching

## AWS Migration Options for Your Existing Account

### Option 1: ðŸš€ AWS RDS PostgreSQL (RECOMMENDED)
**Best Performance**: 20-100ms queries
**Cost**: $20-50/month for small-medium workload

#### Setup Steps:
```bash
# 1. Create RDS instance (using AWS Console or CLI)
aws rds create-db-instance \
  --db-instance-identifier trynex-db \
  --db-instance-class db.t3.micro \
  --engine postgres \
  --master-username postgres \
  --master-user-password YOUR_SECURE_PASSWORD \
  --allocated-storage 20 \
  --vpc-security-group-ids sg-xxxxxxxx \
  --db-subnet-group-name default \
  --backup-retention-period 7 \
  --storage-encrypted

# 2. Configure connection
# New DATABASE_URL format:
# postgresql://postgres:password@trynex-db.xxxxx.ap-southeast-1.rds.amazonaws.com:5432/postgres
```

#### Migration Process:
```bash
# 1. Export from Supabase
pg_dump "postgresql://postgres.lxhhgdqfxmeohayceshb:Amiomito1Amiomito1@aws-0-ap-southeast-1.pooler.supabase.com:6543/postgres" > backup.sql

# 2. Import to AWS RDS
psql "postgresql://postgres:password@your-rds-endpoint:5432/postgres" < backup.sql

# 3. Update environment variable
DATABASE_URL=postgresql://postgres:password@your-rds-endpoint:5432/postgres
```

### Option 2: ðŸ’¾ AWS RDS Aurora Serverless
**Best for**: Variable workloads, auto-scaling
**Cost**: $0.50/hour when active (pauses when idle)

```bash
aws rds create-db-cluster \
  --db-cluster-identifier trynex-aurora \
  --engine aurora-postgresql \
  --engine-mode serverless \
  --master-username postgres \
  --master-user-password YOUR_PASSWORD \
  --scaling-configuration MinCapacity=2,MaxCapacity=4,AutoPause=true,SecondsUntilAutoPause=300
```

### Option 3: âš¡ Amazon ElastiCache + RDS
**Ultra Performance**: Sub-10ms for cached queries
**Cost**: $15-30/month (RDS + ElastiCache)

```typescript
// Add Redis caching layer
import { createClient } from 'redis';

const redis = createClient({
  url: 'redis://your-elasticache-endpoint:6379'
});

async getProducts(): Promise<Product[]> {
  const cacheKey = 'products:all';
  const cached = await redis.get(cacheKey);
  if (cached) return JSON.parse(cached);
  
  const products = await db.select().from(products);
  await redis.setex(cacheKey, 300, JSON.stringify(products)); // 5min cache
  return products;
}
```

## Complete Migration Steps

### Phase 1: Setup AWS RDS
1. **Create RDS Instance**:
   - Instance class: `db.t3.micro` (free tier eligible)
   - Storage: 20GB SSD
   - Multi-AZ: No (for cost savings)
   - Public access: Yes (for Replit access)

2. **Security Group**:
   - Inbound rule: PostgreSQL (5432) from 0.0.0.0/0
   - Or specific Replit IP ranges for security

3. **Get Connection String**:
   ```
   postgresql://postgres:PASSWORD@your-endpoint.ap-southeast-1.rds.amazonaws.com:5432/postgres
   ```

### Phase 2: Data Migration
```bash
# 1. Backup current data
pg_dump "$CURRENT_DATABASE_URL" --clean --create > trynex_backup.sql

# 2. Restore to AWS RDS
psql "$NEW_AWS_DATABASE_URL" < trynex_backup.sql

# 3. Verify data integrity
psql "$NEW_AWS_DATABASE_URL" -c "SELECT COUNT(*) FROM products;"
```

### Phase 3: Update Application
```bash
# 1. Update environment variable in Replit
DATABASE_URL=postgresql://postgres:password@your-rds-endpoint:5432/postgres

# 2. Test connection
npm run db:push

# 3. Restart application
```

### Phase 4: Performance Optimization
```sql
-- Add performance indexes
CREATE INDEX CONCURRENTLY idx_products_category_stock ON products(category, stock) WHERE stock > 0;
CREATE INDEX CONCURRENTLY idx_products_featured_latest ON products(is_featured, is_latest, created_at);
CREATE INDEX CONCURRENTLY idx_orders_status_created ON orders(status, created_at);

-- Enable query logging for monitoring
ALTER SYSTEM SET log_statement = 'all';
ALTER SYSTEM SET log_duration = on;
```

## Cost Comparison

| Option | Monthly Cost | Query Speed | Scalability |
|--------|-------------|-------------|-------------|
| **Current Supabase** | Free | 2000ms+ | Limited |
| **AWS RDS t3.micro** | $15-25 | 20-100ms | Good |
| **Aurora Serverless** | $10-40 | 10-50ms | Excellent |
| **RDS + ElastiCache** | $30-50 | 5-20ms | Excellent |

## Automated Migration Script

I can create an automated migration script that:
1. âœ… Backs up your current Supabase data
2. âœ… Creates AWS RDS instance via CLI
3. âœ… Migrates all data with integrity checks
4. âœ… Updates your Replit environment variables
5. âœ… Runs performance optimizations

**Everything will remain exactly the same** - same tables, same data, same API, just faster performance.

## Next Steps

Would you like me to:

1. **ðŸš€ Start automated AWS RDS migration** (I'll handle everything)
2. **âš¡ Setup the migration script** (you run it when ready)  
3. **ðŸ”§ First optimize current Supabase** (add more caching layers)

**Benefits of AWS Migration**:
- âœ… Keep everything exactly the same
- âœ… 10x faster queries (200ms â†’ 20ms)
- âœ… Better reliability and uptime
- âœ… Dedicated resources (not shared)
- âœ… Advanced monitoring and backups

Let me know your preference and I'll proceed with the migration!